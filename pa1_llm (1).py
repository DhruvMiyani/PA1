# -*- coding: utf-8 -*-
"""PA1_LLM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SMqYNXJLm1bOZUjXk77-qh-qhUSgAR3I

### Dhruv Miyani PA 1 : DS 5983 LLM
"""

pip install nltk

import math

"""# Pre-Processing"""

import nltk
from nltk.corpus import gutenberg
import re

# Downloading resources
nltk.download('gutenberg')
nltk.download('punkt')

# Load corpus
corpus = gutenberg.raw('austen-persuasion.txt')


def preprocessing(corpus):
    # Converting  to lowercase
    corpus = corpus.lower()
    # Removing special characters
    corpus = re.sub(r'[^a-zA-Z0-9\s]', '', corpus)
    return corpus

def tokenizing(corpus):
    #Tokenizing
    return nltk.word_tokenize(corpus)


# Clean and preprocess the text
cleaned_corpus = preprocessing(corpus)

# Tokenize the text
tokens = tokenizing(cleaned_corpus)

print("we have total ", tokens , "  tokens" )
len(tokens)

"""## Modal

$$
P(x_i|x_1, \ldots, x_{i-1}) = P(x_i|x_{i-n+1}, \ldots, x_{i-1}) = \frac{N(x_{i-n+1}, \ldots, x_{i-1}, x_i)}{N(x_{i-n+1}, \ldots, x_{i-1})}
$$

N -GRAM formula: from lecture slides, it based on chain rule
$$
P(x_1, x_2, \ldots, x_n) = P(x_1)P(x_2|x_1)P(x_3|x_1, x_2) \ldots P(x_n|x_1, \ldots, x_{n-1}) = \prod_{i=1}^{n} P(x_i|x_{<i})
$$
"""

import random
from collections import defaultdict, Counter
import itertools




def generate_ngrams(tokens, n):
    #It will Generate n-grams from the list of tokens

    ngrams = zip(*[tokens[i:] for i in range(n)])
    return [" ".join(ngram) for ngram in ngrams]


tokens = tokenizing(cleaned_corpus)  # Use the tokenize_text function from previous code
trigrams = generate_ngrams(tokens, 3)

trigrams[:20]

"""Calculate_frequencies function is counting how many times each n-gram appears in the list. This is useful for determining the most common n-grams in your corpus of text."""

def calculate_frequencies(ngrams):
    return Counter(ngrams)


trigram_freqs = calculate_frequencies(trigrams)
print(trigram_freqs)



"""It then creates a dictionary comprehension that iterates through each n-gram and its count, calculating the probability of that n-gram by dividing its count by the total count of all n-grams.
The output is a dictionary where each n-gram is a key, and its probability is the corresponding value.
"""

def calculate_probabilities(ngrams):

    total_count = sum(ngrams.values())
    return {ngram: count / total_count for ngram, count in ngrams.items()}

trigram_freqs_list = list(trigram_freqs.items())[:30]
probabilities = calculate_probabilities(dict(trigram_freqs_list))

probabilities

def next_word(prefix, ngrams, n):
    prefix = " ".join(prefix.split()[-(n-1):])
    candidates = {ngram: count for ngram, count in ngrams.items() if ngram.startswith(prefix)}
    if not candidates:
        return random.choice(list(ngrams.keys())).split()[-1]
    return max(candidates, key=candidates.get).split()[-1]

def generate_sentence(prefix, length, n, ngrams):

    #Generating a sentence

     for _ in range(length):
        predicted_next_word = predict_next_word(prefix, ngrams, n)  # Using 'predicted_next_word' to avoid conflict
        prefix += " " + predicted_next_word
     return prefix

#lapace smoothing : Assigning a probability of zero to unseen events can cause issues in language models,
# so we use laplace smoothing

def smoothing(ngrams, vocabulary_size, alpha=1):
    #Apply Laplace smoothing to the n-gram probabilities.
    total_count = sum(ngrams.values()) + alpha * vocabulary_size
    return {ngram: (count + alpha) / total_count for ngram, count in ngrams.items()}



# Example usage with n=3
tokens = tokenizing(cleaned_text)  # Use the tokenize_text function from previous code
trigrams = generate_ngrams(tokens, 3)
trigram_freqs = calculate_frequencies(trigrams)
trigram_probs = calculate_probabilities(trigram_freqs)

# For smoothing
vocabulary_size = len(set(tokens))
smoothed_probs = smoothing(trigram_freqs, vocabulary_size)

# Predicting next word
prefix = "she will "
p_next_word = next_word(prefix, smoothed_probs, 3)
print("Next word:", p_next_word)

# Generating a sentence
generated_sentence = generate_sentence(prefix, 10, 3, smoothed_probs)
print("Generated sentence:", generated_sentence)

# Example usage with n=3
tokens = tokenizing(cleaned_text)  # Assuming 'tokenizing' is your function for tokenization
trigrams = generate_ngrams(tokens, 3)
trigram_freqs = calculate_frequencies(trigrams)
trigram_probs = calculate_probabilities(trigram_freqs)

# For smoothing
vocabulary_size = len(set(tokens))
smoothed_probs = smoothing(trigram_freqs, vocabulary_size)  # Assuming 'smoothing' is your smoothing function

# Predicting next word
prefix = "she will "

# Check if you have a function named 'predict_next_word' or similar for predicting the next word
# Make sure that 'next_word' is not used elsewhere as a variable
predicted_next_word = predict_next_word(prefix, smoothed_probs, 3)  # Using 'predicted_next_word' to avoid conflict
print("Next word:", predicted_next_word)

# Generating a sentence
generated_sentence = generate_sentence(prefix, 10, 3, smoothed_probs)
print("Generated sentence:", generated_sentence)

# n=2
tokens = tokenizing(cleaned_corpus)
bigrams = generate_ngrams(tokens, 2)
bigram_freqs = calculate_frequencies(bigrams)
bigram_probs = calculate_probabilities(bigram_freqs)

# For smoothing
vocabulary_size = len(set(tokens))
smoothed_probs = smoothing(bigram_freqs, vocabulary_size)

# Predicting next word
prefix = "he is"
next_word = next_word(prefix, smoothed_probs, 2)
print("Next word:", next_word)

# Generating a sentence
generated_sentence = generate_sentence(prefix, 10, 2, smoothed_probs)
print("Generated sentence:", generated_sentence)

"""#Modal Testing"""



def test_model(n, test_prefixes):
    ngrams = generate_ngrams(tokens, n)
    ngram_freqs = calculate_frequencies(ngrams)
    smoothed_probs = smoothing(ngram_freqs, len(set(tokens)))

    for prefix in test_prefixes:
        predicted_word = predict_next_word(prefix, smoothed_probs, n)  # Changed to 'predicted_word'
        print(f"For prefix '{prefix}', the predicted next word is '{predicted_word}'")

# Test prefixes for bigrams and trigrams
test_prefixes_bigrams = ["she was", "in the", "to be"]
test_prefixes_trigrams = ["she was very", "in the middle", "to be or"]
test_prefixes_4ngrams = ["she was very good", "in the middle of", "see he is quite"]

print("Testing Bigrams:")
test_model(2, test_prefixes_bigrams)

print("\nTesting Trigrams:")
test_model(3, test_prefixes_trigrams)

print("\nTesting 4-grams:")
test_model(4, test_prefixes_4ngrams)



def calculate_perplexity(test_tokens, ngram_freqs, vocabulary_size, n):

    # Generate n-grams from the test tokens
    test_ngrams = generate_ngrams(test_tokens, n)
    # Calculate total number of test n-grams for normalization
    N = len(test_ngrams)

    # Calculate smoothed probabilities for n-grams
    smoothed_probs = smoothing(ngram_freqs, vocabulary_size)

    # Calculate perplexity using log probabilities to avoid underflow
    log_probability_sum = 0
    for ngram in test_ngrams:
        if ngram in smoothed_probs:
            log_probability_sum += math.log(smoothed_probs[ngram])
        else:
            # Use a smoothed probability for unseen n-grams
            log_probability_sum += math.log(1 / (sum(ngram_freqs.values()) + vocabulary_size))

    perplexity = math.exp(-log_probability_sum / N)
    return perplexity


test_text = gutenberg.raw('austen-persuasion.txt')
test_cleaned_text = preprocessing(test_text)
test_tokens = tokenizing(test_cleaned_text)

ngram_freqs = calculate_frequencies(generate_ngrams(tokens, 3))

vocabulary_size = len(set(tokens))



perplexity = calculate_perplexity(test_tokens, ngram_freqs, vocabulary_size, 3)
print(f"The perplexity of the model is: {perplexity}")

import matplotlib.pyplot as plt
import math

# Assuming all the necessary functions (preprocessing, tokenizing, generate_ngrams, calculate_frequencies, smoothing) are defined

test_text = gutenberg.raw('austen-persuasion.txt')
test_cleaned_text = preprocessing(test_text)
test_tokens = tokenizing(test_cleaned_text)

# Store perplexities for different n-gram values
n_values = range(1, 31)
perplexities = []

for n in n_values:
    ngram_freqs = calculate_frequencies(generate_ngrams(tokens, n))
    vocabulary_size = len(set(tokens))

    perplexity = calculate_perplexity(test_tokens, ngram_freqs, vocabulary_size, n)
    perplexities.append(perplexity)
    print(f"Perplexity for n={n}: {perplexity}")

# Plotting the perplexities
plt.figure(figsize=(12, 6))
plt.plot(n_values, perplexities, marker='o')
plt.xlabel('N-gram value (n)')
plt.ylabel('Perplexity')
plt.title('Perplexity for Different N-gram Values')
plt.xticks(n_values)
plt.grid(True)
plt.show()

# Extra credit

def user():
    while True:
        # User inputs
        prefix = input("Enter a prefix or 0 to quit ")
        if prefix.lower() == '0':
            break

        try:
            length = int(input("Enter length of completion: "))
        except ValueError:
            print("Please enter a valid number.")
            continue

        n = len(prefix.split()) + 1  # Determine n for n-gram model based on prefix length

        # Generate and display sentence completion
        ngrams = generate_ngrams(tokens, n)
        ngram_freqs = calculate_frequencies(ngrams)
        smoothed_probs = smoothing(ngram_freqs, len(set(tokens)))

        completed_sentence = generate_sentence(prefix, length, n, smoothed_probs)
        print("Completed Sentence:", completed_sentence)

# Run the user interface
user()